{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Hands-On Lab \u2014 Ames Housing Dataset *(Student Notebook)*\n\n**Master's in Data Science \u2014 LUISS Guido Carli**\n\n| | |\n|---|---|\n| **Format** | Hands-on practical lab (~90 min) |\n| **Dataset** | Ames Housing \u2014 1,460 houses, 81 features, target: SalePrice |\n| **Slides** | EDA_Lecture.pptx \u2014 use as reference throughout |\n| **Goal** | Execute a complete, systematic EDA workflow from raw data to modeling-ready features |\n\n### How this notebook works\n- **Pre-filled cells** \u2192 run them as-is (setup, helpers, visualizations)\n- **TASK cells** \u2192 you write the code! Follow the instructions and slide references\n- ** Hints** \u2192 nudge you in the right direction\n- Tasks are numbered sequentially. Try before looking at the solution notebook!\n\n| Step | What | Time | Slides |\n|------|------|------|--------|\n| 0 | Setup & Load | 3 min | 2 |\n| 1 | First Contact \u2014 shape, types, sanity | 7 min | 4-6 |\n| 2 | Data Types \u2014 numerical vs categorical | 7 min | 8-9 |\n| 3 | Summary Statistics \u2014 center, spread, shape | 10 min | 11-16 |\n| 4 | Univariate Analysis \u2014 distributions one at a time | 12 min | 18-22 |\n| 5 | Bivariate Analysis \u2014 relationships between pairs | 15 min | 24-29 |\n| 6 | Multivariate \u2014 PCA, pair plots | 8 min | 31-33 |\n| 7 | Missing Data \u2014 patterns & imputation | 10 min | 36-38 |\n| 8 | Outliers \u2014 detection & strategy | 8 min | 40-42 |\n| 9 | Transformations \u2014 log, scaling, encoding | 8 min | 44-46 |\n| 10 | Wrap-up \u2014 checklist & Anscombe | 2 min | 48-53 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## Step 0 \u2014 Setup & Data Loading `[Slide 2]` 3 min"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport warnings\n\nwarnings.filterwarnings('ignore')\nsns.set_theme(style='whitegrid', font_scale=1.05, palette='muted')\nplt.rcParams['figure.dpi'] = 120\nplt.rcParams['figure.figsize'] = (10, 5)\npd.set_option('display.max_columns', 40)\npd.set_option('display.float_format', '{:.2f}'.format)\n\n# Color palette\nNAVY, STEEL, CORAL, GREEN = '#1A3764', '#4682B4', '#E8735A', '#27AE60'\n\nprint(' Setup complete')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load Ames Housing dataset\nfrom sklearn.datasets import fetch_openml\names = fetch_openml(name='house_prices', as_frame=True, parser='auto')\ndf = ames.frame.copy()\nprint(f'Loaded: {df.shape[0]:,} rows \u00d7 {df.shape[1]} columns')\ndf.head(3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## Step 1 \u2014 First Contact with the Data `[Slides 4-6]` 7 min\n\n> *\"Let the data speak \u2014 don't impose assumptions prematurely.\"* \u2014 Tukey\n\nBefore any analysis: **understand what you have**. Shape, types, basic sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 1 \n# Print the shape, memory usage, and first 20 column names of df\n# Hint: Use df.shape, df.memory_usage(deep=True).sum(), df.columns[:20]\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 2 \n# Check for duplicate rows and verify the Id column is unique\n# Hint: df.duplicated().sum() and df['Id'].nunique()\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 3 \n# Print min, max, mean, median, and missing count for SalePrice\n# Hint: Use df['SalePrice'].min(), .max(), .mean(), .median(), .isnull().sum()\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaway:** Mean > Median \u2192 right-skewed distribution (expensive homes pull the mean up). See `[Slide 11]` for why this matters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## Step 2 \u2014 Data Types `[Slides 8-9]` 7 min\n\nChoosing the right visualization and statistical test **depends entirely on data type**.\n\n| | Continuous | Discrete | Nominal | Ordinal |\n|---|---|---|---|---|\n| Example | SalePrice | Bedrooms | BldgType | OverallQual |\n| Plot | Histogram | Bar | Count plot | Ordered bar |\n| Correlation | Pearson/Spearman | Spearman | Chi\u00b2 | Spearman |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 4 \n# Count dtypes, separate numerical vs categorical columns, remove 'Id' from num_cols\n# Hint: df.dtypes.value_counts(), df.select_dtypes(include=[np.number]).columns\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 5 \n# Create 3 side-by-side plots: histogram for GrLivArea, bar for FullBath, barh for MSZoning\n# Hint: plt.subplots(1,3), .hist(), .value_counts().plot.bar(), .value_counts().plot.barh()\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Pandas dtype \u2260 statistical type!** A zip code stored as `int64` is categorical. Always verify with domain knowledge. See `[Slide 9]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## Step 3 \u2014 Summary Statistics `[Slides 11-16]` 10 min\n\nThree pillars: **center** (mean, median, mode), **spread** (std, IQR, CV), **shape** (skewness, kurtosis)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 6 \n# Compute mean, median, mode of SalePrice. Plot histogram with mean/median lines + box plot. [Slide 11]\n# Hint: price.mean(), .median(), .mode()[0]. Use ax.axvline() for vertical lines. ax.boxplot() for box plot\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 7 \n# Compute Mean, Std, IQR, Range, and CV% for 5 features. Which has highest relative variability?\n# Hint: IQR = .quantile(0.75) - .quantile(0.25), CV = std/mean * 100\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 8 \n# Compute skewness and kurtosis for all numerical features. How many have |skew| > 1?\n# Hint: df[num_cols].skew(), .kurtosis(). Flag with .abs() > 1\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 9 \n# Run df.describe() on the first 10 numerical columns. Transpose the result for readability.\n# Hint: df[num_cols[:10]].describe().T\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## Step 4 \u2014 Univariate Analysis `[Slides 18-22]` 12 min\n\nExamine **one variable at a time**. Four views for numerical, two for categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Numerical: Four Views of SalePrice `[Slide 18]`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 10 \n# Create a 2\u00d72 grid showing SalePrice as: histogram, KDE, box plot, violin plot [Slide 18]\n# Hint: plt.subplots(2,2). Use .hist(), .plot.kde(), ax.boxplot(vert=False), ax.violinplot(vert=False)\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Bin Size Effect `[Slide 19]`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 11 \n# Plot SalePrice histograms with 5, 20, 50, and 200 bins side by side. Which reveals the most structure?\n# Hint: plt.subplots(1,4), loop over [5, 20, 50, 200]\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 QQ-Plot: Testing Normality `[Slide 21]`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 12 \n# Create QQ-plots for SalePrice (original) and log(SalePrice). Run Shapiro-Wilk on both.\n# Hint: stats.probplot(data, plot=ax). stats.shapiro(sample). Use np.log1p() for log(x+1)\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Categorical Features `[Slide 22]`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 13 \n# Plot: top 10 neighborhoods (barh), OverallQual distribution (bar), cardinality of all categoricals\n# Hint: value_counts().head(10).plot.barh(), nunique().sort_values()\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## Step 5 \u2014 Bivariate Analysis `[Slides 24-29]` 15 min\n\nRelationships between **pairs** of variables. This is where modeling insights emerge.\n\n### 5.1 Numerical \u00d7 Numerical: Scatter + Pearson `[Slide 24]`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 14 \n# Compute correlation of all numerical features with SalePrice. Show top 8 positive and top 3 negative.\n# Hint: df[num_cols].corrwith(df['SalePrice']).sort_values(ascending=False)\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 15 \n# Scatter plots with regression line for top 4 predictors of SalePrice\n# Hint: np.polyfit() for regression, np.polyval() to evaluate. ax.scatter() + ax.plot()\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Correlation Heatmap + Spearman `[Slides 25-26]`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 16 \n# Create correlation heatmap (lower triangle) for top 8 features. Detect multicollinearity (r > 0.8).\n# Hint: np.triu() for mask, sns.heatmap(mask=mask, annot=True). Loop over pairs to find |r| > 0.8\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 17 \n# Compare Pearson vs Spearman for each top feature. Which features show non-linear relationships?\n# Hint: df[feat].corr(df['SalePrice'], method='pearson') vs method='spearman'\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Categorical \u00d7 Numerical: ANOVA `[Slides 27-28]`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 18 \n# Box plots: SalePrice by ExterQual (ordered) and by top 8 Neighborhoods. Run ANOVA on ExterQual groups.\n# Hint: sns.boxplot(order=...). stats.f_oneway(*groups) for ANOVA. [Slides 27-28]\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Categorical \u00d7 Categorical: Chi\u00b2 `[Slide 29]`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 19 \n# Create contingency table for MSZoning \u00d7 BldgType. Run chi-squared test. Heatmap observed vs expected.\n# Hint: pd.crosstab(). stats.chi2_contingency(ct) returns chi2, p, dof, expected. [Slide 29]\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## Step 6 \u2014 Multivariate Analysis `[Slides 31-33]` 8 min\n\n### 6.1 Pair Plot `[Slide 31]`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 20 \n# Pair plot of SalePrice, GrLivArea, YearBuilt, TotalBsmtSF, colored by quality bins\n# Hint: pd.cut() to bin OverallQual into 4 groups, sns.pairplot(hue=...)\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 PCA `[Slides 32-33]`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 21 \n# PCA: scale data \u2192 fit 10 components \u2192 plot 2D projection colored by SalePrice + scree plot\n# Hint: StandardScaler().fit_transform(), PCA(n_components=10).fit_transform(). Scree: cumsum of explained_variance_ratio_\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## Step 7 \u2014 Missing Data `[Slides 36-38]` 10 min\n\n> **The pattern of missingness contains information!** \u2014 See Rubin's taxonomy `[Slide 36]`\n\n| Type | Mechanism | Strategy |\n|------|-----------|----------|\n| **MCAR** | Independent of all data | Drop rows |\n| **MAR** | Depends on *observed* data | Impute |\n| **MNAR** | Depends on *missing value itself* | Domain knowledge |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 22 \n# Count features with missing values. Show top 10 with their % missing.\n# Hint: df.isnull().sum() / len(df) * 100. Sort descending.\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 23 \n# Bar chart of top 12 missing features. Heatmap of missingness patterns. What co-occurrence patterns do you see?\n# Hint: df[cols].isnull().astype(int) for the matrix. sns.heatmap() with binary colormap. [Slide 37]\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 24 \n# Compare Mean, Median, and KNN imputation on LotFrontage. Which preserves the distribution best?\n# Hint: KNNImputer(n_neighbors=5) from sklearn.impute. Use nearby features (LotArea, GrLivArea). [Slide 38]\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## Step 8 \u2014 Outlier Detection `[Slides 40-42]` 8 min\n\n### 8.1 IQR Method & Z-Score `[Slide 40]`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 25 \n# Write IQR and Z-score outlier functions. Apply to GrLivArea. Scatter plot showing outliers in red.\n# Hint: IQR: Q1-1.5*IQR, Q3+1.5*IQR. Z-score: |z| > 3. [Slide 40]\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Isolation Forest `[Slide 41]`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 26 \n# Isolation Forest on GrLivArea \u00d7 SalePrice (contamination=0.02). Scatter normal vs outlier.\n# Hint: IsolationForest(contamination=0.02). fit_predict() returns 1 (normal) or -1 (outlier). [Slide 41]\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## Step 9 \u2014 Data Transformations `[Slides 44-46]` 8 min\n\n### 9.1 Log Transform `[Slide 44]`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 27 \n# Apply log1p transform to SalePrice, LotArea, GrLivArea. Show before/after with skewness values.\n# Hint: np.log1p(). .skew() for skewness. Side-by-side histograms. [Slide 44]\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Feature Scaling `[Slide 45]`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 28 \n# Apply 4 scalers to GrLivArea. Side-by-side histograms. Which preserves shape? Which normalizes?\n# Hint: StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer from sklearn.preprocessing. [Slide 45]\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Encoding `[Slide 46]`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TASK 29 \n# Apply label encoding to ExterQual, one-hot to BldgType, target encoding to Neighborhood.\n# Hint: .map(dict) for label. pd.get_dummies() for one-hot. .groupby().mean() for target. [Slide 46]\n#\n# YOUR CODE HERE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## Step 10 \u2014 Wrap-up `[Slides 48-53]` 2 min\n\n### Anscombe's Quartet \u2014 Why We ALWAYS Visualize `[Slide 48]`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Anscombe's Quartet \nanscombe = sns.load_dataset('anscombe')\n\nprint('Four datasets with IDENTICAL statistics:')\nfor ds in ['I', 'II', 'III', 'IV']:\n d = anscombe[anscombe['dataset'] == ds]\n print(f' Dataset {ds}: mean(x)={d[\"x\"].mean():.1f}, mean(y)={d[\"y\"].mean():.2f}, '\n f'r={d[\"x\"].corr(d[\"y\"]):.3f}')\n\ng = sns.lmplot(data=anscombe, x='x', y='y', col='dataset', col_wrap=2,\n height=3, aspect=1.3, scatter_kws={'s': 40, 'color': STEEL},\n line_kws={'color': 'red', 'lw': 2})\ng.fig.suptitle('Same Statistics \u2192 Completely Different Patterns!', fontweight='bold', color=CORAL, fontsize=14, y=1.02)\nplt.show()\nprint('\\n LESSON: Never skip visualization. Statistics can lie. [Slide 48]')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA Checklist \u2014 Before You Model `[Slide 53]`\n\nRun through this checklist before starting any modeling:\n\n- [ ] Shape, types, and column names understood?\n- [ ] Missing value patterns identified and strategy decided?\n- [ ] Every feature's distribution examined?\n- [ ] Outliers detected and handling strategy chosen?\n- [ ] Feature-target correlations verified?\n- [ ] Multicollinearity addressed?\n- [ ] Transformations applied (log, scaling)?\n- [ ] Categorical features encoded?\n- [ ] All findings documented?\n\n> **Only when all boxes are checked \u2192 proceed to modeling.**\n\n---\n\n### Summary of what we did today\n\n| Step | What | Key Finding (Ames) |\n|------|------|--------------------|\n| 1 | First contact | 1,460 \u00d7 81, no duplicates |\n| 2 | Data types | 38 numerical, 43 categorical |\n| 3 | Summary stats | SalePrice right-skewed (skew \u2248 1.9) |\n| 4 | Univariate | Many features highly skewed; QQ confirms non-normality |\n| 5 | Bivariate | OverallQual (r=0.79) and GrLivArea (r=0.71) top predictors |\n| 6 | Multivariate | 6 PCs capture 90% variance; quality separates clusters |\n| 7 | Missing data | 19 features missing; PoolQC = 99.5% (no pool, not error) |\n| 8 | Outliers | 2 extreme GrLivArea points; Isolation Forest detects multivariate |\n| 9 | Transforms | Log reduces skew from 1.9 \u2192 0.1; RobustScaler for outliers |\n\n---\n*End of Hands-On Lab \u2014 LUISS Guido Carli*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n### Well done\nYou've completed the full EDA pipeline. Compare your solutions with the complete notebook to check your work.\n\n**Key takeaway:** This workflow (Steps 1\u21929) works on **any** tabular dataset. Memorize the structure, not the code."
   ]
  }
 ]
}